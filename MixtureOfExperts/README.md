# Mixture of Experts

Mixture of Experts (MoE) is a technique that allows a model to use different experts to process different parts of the input.
