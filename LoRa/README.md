# LoRA (Low-Rank Adaptation)

LoRA is a technique to fine-tune a model without retraining the whole model.

A matrix with low rank can be approximated by a product of two smaller matrices. For example a position of a person needs only 2 dimensions, but where a plane is 3 dimensions.

A "low-rank" matrix is a matrix with a small number of rows and columns.
